{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "from tensorflow.python.keras.layers import Input, Dense, Lambda, Flatten, Reshape, Concatenate\n",
    "from tensorflow.python.keras.layers import Conv2D, Conv2DTranspose, BatchNormalization, Activation, LeakyReLU\n",
    "from tensorflow.python.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.python.keras import metrics\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.python.keras.datasets import mnist\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-4-b4f065f01db4>, line 99)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-b4f065f01db4>\"\u001b[0;36m, line \u001b[0;32m99\u001b[0m\n\u001b[0;31m    def get_generator_trained_model(self):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#tensorflow.version\n",
    "\n",
    "def lr_scheduler(initial_lr=1e-3, decay_factor=0.75, step_size=5, min_lr=1e-5):\n",
    "    '''\n",
    "    Wrapper function to create a LearningRateScheduler with step decay schedule.\n",
    "    '''\n",
    "    def schedule(epoch):\n",
    "        lr = initial_lr * (decay_factor ** np.floor(epoch / step_size))\n",
    "        if lr > min_lr:\n",
    "            return lr\n",
    "        return min_lr\n",
    "\n",
    "    return LearningRateScheduler(schedule, verbose=1)\n",
    "\n",
    "\n",
    "class Autoencoder():\n",
    "    def __init__(self, shape_data = (11,1), intermediate_dim=128, latent_dim=32):\n",
    "        self.data_size = shape_data\n",
    "        #self.intermediate_dim = intermediate_dim\n",
    "        #self.latent_dim = latent_dim\n",
    "        self.input = Input(shape=self.data_size)\n",
    "        #self.params = {\n",
    "#             'enc_filters': [16, 32, 48, 16, 32],\n",
    "#             'enc_kernels': [3, 3, 3, 4, 1],\n",
    "#             'enc_strides': [2, 2, 2, 1, 1],\n",
    "#             'dec_filters': [16, 16, 16, 32],\n",
    "#             'dec_kernels': [4, 3, 3, 3],\n",
    "#             'dec_strides': [1, 2, 2, 2],\n",
    "#         }\n",
    "        self.reconstruction_shape = []\n",
    "        self.params = {'enc_dim': [8, 4] , 'dec_dim': [4, 8, 11] }\n",
    "\n",
    "    def build_encoder(self, dims=[8, 4]): #, kernels=[3, 3, 3, 4, 1], strides=[2, 2, 2, 1, 1]):\n",
    "        def f(x):\n",
    "            for intermediate_dim in dims: #num_filter, kernel, stride in zip(filters, kernels, strides):\n",
    "                x = Dense( units=intermediate_dim,\n",
    "                           activation=tf.nn.sigmoid)(x)\n",
    "                x = BatchNormalization()(x)\n",
    "                self.reconstruction_shape += [x.get_shape().as_list()]\n",
    "            return x\n",
    "        return f\n",
    "\n",
    "    def build_decoder(self, dims=[4, 8, 11]): #filters=[64, 64, 64, 32], kernels=[4, 3, 3, 3], strides=[1, 2, 2, 2]):\n",
    "        def f(x):\n",
    "            for intermediate_dim in dims: #for i, (num_filter, kernel, stride) in enumerate(zip(filters, kernels, strides)):\n",
    "                x = Dense(units=intermediate_dim,\n",
    "                           activation=tf.nn.sigmoid)(x)\n",
    "                x = BatchNormalization()(x)\n",
    "                decoder = x\n",
    "            return decoder\n",
    "        return f\n",
    "\n",
    "    def build_model(self):\n",
    "        hidden = self.build_encoder(\n",
    "            dims=self.params['enc_dim'])(self.input)\n",
    "        dec = self.build_decoder(\n",
    "            dims=self.params['dec_dim'])(hidden)\n",
    "        # instantiate VAE model\n",
    "        vae = Model(self.input, dec)\n",
    "        return vae\n",
    "\n",
    "\n",
    "class ADAE(object):\n",
    "    def __init__(self, data_size = (11, 1)):#image_size=(28, 28, 1), latent_dim=100):\n",
    "        self.data_size = data_size\n",
    "        #self.latent_dim = latent_dim\n",
    "\n",
    "        self.input1 = Input(shape=data_size)\n",
    "        self.input2 = Input(shape=data_size)\n",
    "        \n",
    "        # Build the generator\n",
    "        self.generator = Autoencoder(shape_data = data_size).build_model()\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = Autoencoder(shape_data = data_size).build_model()\n",
    "\n",
    "        self.gx1 = self.generator(self.input1)      # g(x1)\n",
    "        self.dx2 = self.discriminator(self.input2)  # d(x2)\n",
    "        self.dgx1 = self.discriminator(self.gx)    # d(g(x1))\n",
    "        \n",
    "        #print(self.gx.shape, self.dx.shape, self.dgx.shape, self.input.shape )\n",
    "        self.d_loss = Lambda(lambda x: K.mean(K.abs(x[0] - x[1]), axis=1) -\n",
    "                             K.mean(K.abs(x[2] - x[3]), axis=1), name='d_loss')([self.input2, self.dx2, self.gx1, self.dgx1])\n",
    "        \n",
    "        self.g_loss = Lambda(lambda x: K.mean(K.abs(x[0] - x[1]), axis=1) +\n",
    "                            K.mean(K.abs(x[1] - x[2]), axis=1), name='g_loss')([self.input1, self.gx1, self.dgx1])\n",
    "\n",
    "        self.model = Model(inputs=[self.input1, self.input2], outputs=[self.g_loss, self.d_loss])\n",
    "        self.model.summary()\n",
    "        # self.generator.summary()\n",
    "        # self.discriminator.summary()\n",
    "\n",
    "    def get_anomaly_score(self):\n",
    "#         \"\"\" Compute the anomaly score. Call it after training. \"\"\"\n",
    "#         score_out = Lambda(lambda x:\n",
    "#                            K.mean(K.mean(K.mean((x[0] - x[1]) ** 2, axis=1), axis=1), axis=1)\n",
    "#                            )([self.model.inputs[0], self.model.layers[2](self.model.layers[1](self.model.inputs[0]))])\n",
    "#         return Model(self.model.inputs[0], score_out)\n",
    "\n",
    "    def get_generator_trained_model(self):\n",
    "        \"\"\" Get the generator to reconstruct the input. Call it after training. \"\"\"\n",
    "        return Model(self.model.inputs[0], self.model.layers[1](self.model.inputs[0]))\n",
    "\n",
    "    def get_discrinminator_trained_model(self):\n",
    "        \"\"\" Get the discrinminator to reconstruct the input. Call it after training. \"\"\"\n",
    "        return Model(self.model.inputs[0], self.model.layers[2](self.model.layers[1](self.model.inputs[0])))\n",
    "\n",
    "    def train(self, x1_train, x2_train, x1_test, x2_test, epochs=1):\n",
    "        self.model.add_loss(K.mean(self.g_loss))\n",
    "        self.model.add_metric(self.g_loss, aggregation='mean', name=\"g_loss\")\n",
    "        self.model.add_loss(K.mean(self.d_loss))\n",
    "        self.model.add_metric(self.d_loss, aggregation='mean', name=\"d_loss\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print('Epoch %d/%d' % (epoch + 1, epochs))\n",
    "            # Train generator only\n",
    "            self.model.layers[1].trainable = True\n",
    "            self.model.layers[2].trainable = False\n",
    "            self.model.compile('adam', loss_weights={'g_loss': 1, 'd_loss': 0})\n",
    "            print('Training on Generator')\n",
    "            self.model.fit(\n",
    "                [x1_train, x2_train],\n",
    "                batch_size=10, #64,\n",
    "                steps_per_epoch=10, #200,\n",
    "                epochs=epoch,\n",
    "                callbacks=[\n",
    "                        lr_scheduler(initial_lr=1e-3, decay_factor=0.75, step_size=10, min_lr=1e-5)\n",
    "                ],\n",
    "                initial_epoch=epoch - 1\n",
    "            )\n",
    "            # Train discriminator only\n",
    "            self.model.layers[1].trainable = False\n",
    "            self.model.layers[2].trainable = True\n",
    "            self.model.compile('adam', loss_weights={'g_loss': 0, 'd_loss': 1})\n",
    "            print('Training on Discriminator')\n",
    "            self.model.fit(\n",
    "                [x1_train, x2_train],\n",
    "                batch_size=10, #64,\n",
    "                steps_per_epoch=10, #200,\n",
    "                epochs=epoch,\n",
    "                callbacks=[\n",
    "                    ModelCheckpoint(\n",
    "                        'model_checkpoint/model_%d_gloss_{g_loss:.4f}_dloss_{d_loss:.4f}.h5' % epoch, \n",
    "                        verbose=1),\n",
    "                    lr_scheduler(initial_lr=1e-3, decay_factor=0.75, step_size=10, min_lr=1e-5)\n",
    "                ],\n",
    "                initial_epoch=epoch - 1\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \n",
    "path = '/Users/laurieprelot/Documents/Projects/2019_Deep_learning/data/Chevrier-et-al/chevrier_data_pooled_nona.parquet'\n",
    "#path = 'chevrier_data_pooled_nona.parquet'\n",
    "chve = pd.read_parquet(path, engine='pyarrow')\n",
    "chve.shape\n",
    "#np.random.seed(123456)\n",
    "#idx = np.random.choice(a = np.arange(chve.shape[0]), size = 2000, replace = False)\n",
    "#chve_s = #chve.iloc[idx, ]\n",
    "ID = 'rcc7'\n",
    "select_cols = [col for col in chve.columns if not \"metadata\" in col]\n",
    "chve = chve.loc[:,select_cols]\n",
    "chve_s_patient = chve.reset_index()\n",
    "chve_s_patient = chve_s_patient.rename({'level_0':'batch', 'level_1':'patient', 'level_2':'cell'} , axis = 1)\n",
    "chve_s_patient = chve_s_patient.loc[chve_s_patient['patient'] == ID, :]\n",
    "\n",
    "chve_s_patient_batch1 = chve_s_patient.loc[ chve_s_patient['batch'] == \"experiment_101725_files\"]\n",
    "chve_s_patient_batch2 = chve_s_patient.loc[ chve_s_patient['batch'] == \"experiment_102007_files\"]\n",
    "\n",
    "# target, split\n",
    "chve_s_patient_batch1 = chve_s_patient_batch1.iloc[1:1000,:]\n",
    "chve_s_patient_batch2 = chve_s_patient_batch2.iloc[1:1000,:]\n",
    "y = chve_s_patient_batch1[\"batch\"]\n",
    "x1 = chve_s_patient_batch1.drop([\"batch\", 'cell', 'patient'], axis = 1) \n",
    "x2 = chve_s_patient_batch2.drop([\"batch\", 'cell', 'patient'], axis = 1) \n",
    "x1_train, x1_test, x2_train, x2_test = train_test_split(x1, x2, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adae = ADAE(data_size=(11,))\n",
    "adae.train(x1_train.values, x2_train.values, x1_test.values, x2_test.values, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(x_train.values))\n",
    "print(np.shape(y_train.values))\n",
    "print(np.shape(x_test.values))\n",
    "print(np.shape(y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312    experiment_101725_files\n",
       "606    experiment_101725_files\n",
       "440    experiment_101725_files\n",
       "1      experiment_101725_files\n",
       "317    experiment_101725_files\n",
       "                ...           \n",
       "107    experiment_101725_files\n",
       "271    experiment_101725_files\n",
       "861    experiment_101725_files\n",
       "436    experiment_101725_files\n",
       "103    experiment_101725_files\n",
       "Name: batch, Length: 669, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
