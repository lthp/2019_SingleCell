{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "from tensorflow.python.keras.layers import Input, Dense, Lambda, Flatten, Reshape, Concatenate\n",
    "from tensorflow.python.keras.layers import Conv2D, Conv2DTranspose, BatchNormalization, Activation, LeakyReLU\n",
    "from tensorflow.python.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.python.keras import metrics\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.python.keras.datasets import mnist\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow.version\n",
    "\n",
    "def lr_scheduler(initial_lr=1e-3, decay_factor=0.75, step_size=5, min_lr=1e-5):\n",
    "    '''\n",
    "    Wrapper function to create a LearningRateScheduler with step decay schedule.\n",
    "    '''\n",
    "    def schedule(epoch):\n",
    "        lr = initial_lr * (decay_factor ** np.floor(epoch / step_size))\n",
    "        if lr > min_lr:\n",
    "            return lr\n",
    "        return min_lr\n",
    "\n",
    "    return LearningRateScheduler(schedule, verbose=1)\n",
    "\n",
    "\n",
    "class Autoencoder():\n",
    "    def __init__(self, shape_data = (11,1), intermediate_dim=128, latent_dim=32):\n",
    "        self.data_size = shape_data\n",
    "        #self.intermediate_dim = intermediate_dim\n",
    "        #self.latent_dim = latent_dim\n",
    "        self.input = Input(shape=self.data_size)\n",
    "        #self.params = {\n",
    "#             'enc_filters': [16, 32, 48, 16, 32],\n",
    "#             'enc_kernels': [3, 3, 3, 4, 1],\n",
    "#             'enc_strides': [2, 2, 2, 1, 1],\n",
    "#             'dec_filters': [16, 16, 16, 32],\n",
    "#             'dec_kernels': [4, 3, 3, 3],\n",
    "#             'dec_strides': [1, 2, 2, 2],\n",
    "#         }\n",
    "        self.reconstruction_shape = []\n",
    "        self.params = {'enc_dim': [8, 4] , 'dec_dim': [4, 8, 11] }\n",
    "\n",
    "    def build_encoder(self, dims=[8, 4]): #, kernels=[3, 3, 3, 4, 1], strides=[2, 2, 2, 1, 1]):\n",
    "        def f(x):\n",
    "            for intermediate_dim in dims: #num_filter, kernel, stride in zip(filters, kernels, strides):\n",
    "                x = Dense( units=intermediate_dim,\n",
    "                           activation=tf.nn.sigmoid)(x)\n",
    "                x = BatchNormalization()(x)\n",
    "                self.reconstruction_shape += [x.get_shape().as_list()]\n",
    "            return x\n",
    "        return f\n",
    "\n",
    "    def build_decoder(self, dims=[4, 8, 11]): #filters=[64, 64, 64, 32], kernels=[4, 3, 3, 3], strides=[1, 2, 2, 2]):\n",
    "        def f(x):\n",
    "            for intermediate_dim in dims: #for i, (num_filter, kernel, stride) in enumerate(zip(filters, kernels, strides)):\n",
    "                x = Dense(units=intermediate_dim,\n",
    "                           activation=tf.nn.sigmoid)(x)\n",
    "                x = BatchNormalization()(x)\n",
    "                decoder = x\n",
    "            return decoder\n",
    "        return f\n",
    "\n",
    "    def build_model(self):\n",
    "        hidden = self.build_encoder(\n",
    "            dims=self.params['enc_dim'])(self.input)\n",
    "        dec = self.build_decoder(\n",
    "            dims=self.params['dec_dim'])(hidden)\n",
    "        # instantiate VAE model\n",
    "        vae = Model(self.input, dec)\n",
    "        return vae\n",
    "\n",
    "\n",
    "class ADAE(object):\n",
    "    def __init__(self, data_size = (11, 1)):#image_size=(28, 28, 1), latent_dim=100):\n",
    "        self.data_size = data_size\n",
    "        #self.latent_dim = latent_dim\n",
    "\n",
    "        self.input1 = Input(shape=data_size)\n",
    "        self.input2 = Input(shape=data_size)\n",
    "        \n",
    "        # Build the generator\n",
    "        self.generator = Autoencoder(shape_data = data_size).build_model()\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = Autoencoder(shape_data = data_size).build_model()\n",
    "\n",
    "        self.gx1 = self.generator(self.input1)      # g(x1)\n",
    "        self.dx2 = self.discriminator(self.input2)  # d(x2)\n",
    "        self.dgx1 = self.discriminator(self.gx)    # d(g(x1))\n",
    "        \n",
    "        #print(self.gx.shape, self.dx.shape, self.dgx.shape, self.input.shape )\n",
    "        self.d_loss = Lambda(lambda x: K.mean(K.abs(x[0] - x[1]), axis=1) -\n",
    "                             K.mean(K.abs(x[2] - x[3]), axis=1), name='d_loss')([self.input2, self.dx2, self.gx1, self.dgx1])\n",
    "        \n",
    "        self.g_loss = Lambda(lambda x: K.mean(K.abs(x[0] - x[1]), axis=1) +\n",
    "                            K.mean(K.abs(x[1] - x[2]), axis=1), name='g_loss')([self.input1, self.gx1, self.dgx1])\n",
    "\n",
    "        self.model = Model(inputs=[self.input1, self.input2], outputs=[self.g_loss, self.d_loss])\n",
    "        self.model.summary()\n",
    "        # self.generator.summary()\n",
    "        # self.discriminator.summary()\n",
    "\n",
    "    def get_anomaly_score(self):\n",
    "#         \"\"\" Compute the anomaly score. Call it after training. \"\"\"\n",
    "#         score_out = Lambda(lambda x:\n",
    "#                            K.mean(K.mean(K.mean((x[0] - x[1]) ** 2, axis=1), axis=1), axis=1)\n",
    "#                            )([self.model.inputs[0], self.model.layers[2](self.model.layers[1](self.model.inputs[0]))])\n",
    "#         return Model(self.model.inputs[0], score_out)\n",
    "\n",
    "    def get_generator_trained_model(self):\n",
    "        \"\"\" Get the generator to reconstruct the input. Call it after training. \"\"\"\n",
    "        return Model(self.model.inputs[0], self.model.layers[1](self.model.inputs[0]))\n",
    "\n",
    "    def get_discrinminator_trained_model(self):\n",
    "        \"\"\" Get the discrinminator to reconstruct the input. Call it after training. \"\"\"\n",
    "        return Model(self.model.inputs[0], self.model.layers[2](self.model.layers[1](self.model.inputs[0])))\n",
    "\n",
    "    def train(self, x1_train, x2_train, x1_test, x2_test, epochs=1):\n",
    "        self.model.add_loss(K.mean(self.g_loss))\n",
    "        self.model.add_metric(self.g_loss, aggregation='mean', name=\"g_loss\")\n",
    "        self.model.add_loss(K.mean(self.d_loss))\n",
    "        self.model.add_metric(self.d_loss, aggregation='mean', name=\"d_loss\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print('Epoch %d/%d' % (epoch + 1, epochs))\n",
    "            # Train generator only\n",
    "            self.model.layers[1].trainable = True\n",
    "            self.model.layers[2].trainable = False\n",
    "            self.model.compile('adam', loss_weights={'g_loss': 1, 'd_loss': 0})\n",
    "            print('Training on Generator')\n",
    "            self.model.fit(\n",
    "                [x1_train, x2_train],\n",
    "                batch_size=10, #64,\n",
    "                steps_per_epoch=10, #200,\n",
    "                epochs=epoch,\n",
    "                callbacks=[\n",
    "                        lr_scheduler(initial_lr=1e-3, decay_factor=0.75, step_size=10, min_lr=1e-5)\n",
    "                ],\n",
    "                initial_epoch=epoch - 1\n",
    "            )\n",
    "            # Train discriminator only\n",
    "            self.model.layers[1].trainable = False\n",
    "            self.model.layers[2].trainable = True\n",
    "            self.model.compile('adam', loss_weights={'g_loss': 0, 'd_loss': 1})\n",
    "            print('Training on Discriminator')\n",
    "            self.model.fit(\n",
    "                [x1_train, x2_train],\n",
    "                batch_size=10, #64,\n",
    "                steps_per_epoch=10, #200,\n",
    "                epochs=epoch,\n",
    "                callbacks=[\n",
    "                    ModelCheckpoint(\n",
    "                        'model_checkpoint/model_%d_gloss_{g_loss:.4f}_dloss_{d_loss:.4f}.h5' % epoch, \n",
    "                        verbose=1),\n",
    "                    lr_scheduler(initial_lr=1e-3, decay_factor=0.75, step_size=10, min_lr=1e-5)\n",
    "                ],\n",
    "                initial_epoch=epoch - 1\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \n",
    "path = 'chevrier_data_pooled_nona.parquet'\n",
    "chve = pd.read_parquet(path, engine='pyarrow')\n",
    "chve.shape\n",
    "#np.random.seed(123456)\n",
    "#idx = np.random.choice(a = np.arange(chve.shape[0]), size = 2000, replace = False)\n",
    "#chve_s = #chve.iloc[idx, ]\n",
    "ID = 'rcc7'\n",
    "select_cols = [col for col in chve.columns if not \"metadata\" in col]\n",
    "chve = chve.loc[:,select_cols]\n",
    "chve_s_patient = chve.reset_index()\n",
    "chve_s_patient = chve_s_patient.rename({'level_0':'batch', 'level_1':'patient', 'level_2':'cell'} , axis = 1)\n",
    "chve_s_patient = chve_s_patient.loc[chve_s_patient['patient'] == ID, :]\n",
    "\n",
    "chve_s_patient_batch1 = chve_s_patient.loc[ chve_s_patient['batch'] == \"experiment_101725_files\"]\n",
    "chve_s_patient_batch2 = chve_s_patient.loc[ chve_s_patient['batch'] == \"experiment_102007_files\"]\n",
    "\n",
    "# target, split\n",
    "chve_s_patient_batch1 = chve_s_patient_batch1.iloc[1:1000,:]\n",
    "chve_s_patient_batch2 = chve_s_patient_batch2.iloc[1:1000,:]\n",
    "y = chve_s_patient_batch1[\"batch\"]\n",
    "x1 = chve_s_patient_batch1.drop([\"batch\", 'cell', 'patient'], axis = 1) \n",
    "x2 = chve_s_patient_batch2.drop([\"batch\", 'cell', 'patient'], axis = 1) \n",
    "x1_train, x1_test, x2_train, x2_test = train_test_split(x1, x2, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           [(None, 11)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_12 (Model)                (None, 11)           431         input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           [(None, 11)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_13 (Model)                (None, 11)           431         input_20[0][0]                   \n",
      "                                                                 model_12[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "g_loss (Lambda)                 (None,)              0           input_19[0][0]                   \n",
      "                                                                 model_12[1][0]                   \n",
      "                                                                 model_13[2][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "d_loss (Lambda)                 (None,)              0           input_20[0][0]                   \n",
      "                                                                 model_13[1][0]                   \n",
      "                                                                 model_12[1][0]                   \n",
      "                                                                 model_13[2][0]                   \n",
      "==================================================================================================\n",
      "Total params: 862\n",
      "Trainable params: 722\n",
      "Non-trainable params: 140\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/15\n",
      "WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Generator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00000: LearningRateScheduler reducing learning rate to 0.0013333333333333333.\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525AE419D8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525AE419D8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "100/669 [===>..........................] - ETA: 48s - loss: 52.7165 - g_loss: 21.2032 - d_loss: 31.5134WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Discriminator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00000: LearningRateScheduler reducing learning rate to 0.0013333333333333333.\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525E6E7A60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525E6E7A60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "100/669 [===>..........................] - ETA: 39s - loss: 47.3690 - g_loss: 16.5876 - d_loss: 30.7814\n",
      "Epoch 00000: saving model to model_checkpoint/model_0_gloss_16.5876_dloss_30.7814.h5\n",
      "100/669 [===>..........................] - ETA: 41s - loss: 47.3690 - g_loss: 16.5876 - d_loss: 30.7814Epoch 2/15\n",
      "WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Generator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525DBE2400> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525DBE2400> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "100/669 [===>..........................] - ETA: 46s - loss: 50.0472 - g_loss: 18.9448 - d_loss: 31.1024WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Discriminator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000252618FBA60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000252618FBA60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "100/669 [===>..........................] - ETA: 40s - loss: 47.9391 - g_loss: 16.6545 - d_loss: 31.2846\n",
      "Epoch 00001: saving model to model_checkpoint/model_1_gloss_16.6545_dloss_31.2846.h5\n",
      "100/669 [===>..........................] - ETA: 41s - loss: 47.9391 - g_loss: 16.6545 - d_loss: 31.2846Epoch 3/15\n",
      "WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Generator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 2/2\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525D19C840> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525D19C840> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      " 90/669 [===>..........................] - ETA: 53s - loss: 53.3536 - g_loss: 21.9464 - d_loss: 31.4072 WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Discriminator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 2/2\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525DF2B268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525DF2B268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "100/669 [===>..........................] - ETA: 39s - loss: 44.9493 - g_loss: 15.6477 - d_loss: 29.3016\n",
      "Epoch 00002: saving model to model_checkpoint/model_2_gloss_15.6477_dloss_29.3016.h5\n",
      "100/669 [===>..........................] - ETA: 40s - loss: 44.9493 - g_loss: 15.6477 - d_loss: 29.3016Epoch 4/15\n",
      "WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Generator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 3/3\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000025261CCFA60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000025261CCFA60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      " 90/669 [===>..........................] - ETA: 54s - loss: 46.6365 - g_loss: 15.4109 - d_loss: 31.2256 WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Discriminator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 3/3\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525D344510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525D344510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      " 80/669 [==>...........................] - ETA: 52s - loss: 47.6215 - g_loss: 16.7562 - d_loss: 30.8652 \n",
      "Epoch 00003: saving model to model_checkpoint/model_3_gloss_15.9112_dloss_31.7500.h5\n",
      "100/669 [===>..........................] - ETA: 41s - loss: 47.6612 - g_loss: 15.9112 - d_loss: 31.7500Epoch 5/15\n",
      "WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Generator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 4/4\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525D86BF28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525D86BF28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "100/669 [===>..........................] - ETA: 46s - loss: 47.2787 - g_loss: 18.5173 - d_loss: 28.7615WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Discriminator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 4/4\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525A90CF28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525A90CF28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      " 90/669 [===>..........................] - ETA: 44s - loss: 49.0111 - g_loss: 16.3526 - d_loss: 32.6586\n",
      "Epoch 00004: saving model to model_checkpoint/model_4_gloss_16.0318_dloss_31.7149.h5\n",
      "100/669 [===>..........................] - ETA: 40s - loss: 47.7466 - g_loss: 16.0318 - d_loss: 31.7149Epoch 6/15\n",
      "WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Generator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 5/5\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525DF51B70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525DF51B70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "100/669 [===>..........................] - ETA: 48s - loss: 51.7745 - g_loss: 20.6791 - d_loss: 31.0955WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Discriminator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 5/5\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525D344D08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525D344D08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "100/669 [===>..........................] - ETA: 40s - loss: 47.6831 - g_loss: 18.0910 - d_loss: 29.5921\n",
      "Epoch 00005: saving model to model_checkpoint/model_5_gloss_18.0910_dloss_29.5921.h5\n",
      "100/669 [===>..........................] - ETA: 41s - loss: 47.6831 - g_loss: 18.0910 - d_loss: 29.5921Epoch 7/15\n",
      "WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Generator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 6/6\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525E711950> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525E711950> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      " 90/669 [===>..........................] - ETA: 52s - loss: 45.9348 - g_loss: 17.3777 - d_loss: 28.5571 WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Discriminator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 6/6\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525DF10BF8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525DF10BF8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      " 90/669 [===>..........................] - ETA: 44s - loss: 49.8757 - g_loss: 18.9707 - d_loss: 30.9050\n",
      "Epoch 00006: saving model to model_checkpoint/model_6_gloss_18.6469_dloss_31.0232.h5\n",
      "100/669 [===>..........................] - ETA: 41s - loss: 49.6701 - g_loss: 18.6469 - d_loss: 31.0232Epoch 8/15\n",
      "WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Generator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 7/7\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525A90C6A8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525A90C6A8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      " 90/669 [===>..........................] - ETA: 56s - loss: 48.5536 - g_loss: 18.0825 - d_loss: 30.4711 WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Discriminator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 7/7\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002526112DEA0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002526112DEA0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "100/669 [===>..........................] - ETA: 40s - loss: 49.3032 - g_loss: 18.0561 - d_loss: 31.2471\n",
      "Epoch 00007: saving model to model_checkpoint/model_7_gloss_18.0561_dloss_31.2471.h5\n",
      "100/669 [===>..........................] - ETA: 42s - loss: 49.3032 - g_loss: 18.0561 - d_loss: 31.2471Epoch 9/15\n",
      "WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Generator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 8/8\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525AA0BA60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525AA0BA60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      " 90/669 [===>..........................] - ETA: 52s - loss: 49.9569 - g_loss: 16.8767 - d_loss: 33.0803 WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Discriminator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 8/8\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000025260D6F488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000025260D6F488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      " 90/669 [===>..........................] - ETA: 45s - loss: 49.3166 - g_loss: 18.9831 - d_loss: 30.3335\n",
      "Epoch 00008: saving model to model_checkpoint/model_8_gloss_18.7355_dloss_29.3760.h5\n",
      "100/669 [===>..........................] - ETA: 41s - loss: 48.1115 - g_loss: 18.7355 - d_loss: 29.3760Epoch 10/15\n",
      "WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Generator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 9/9\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525D344378> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525D344378> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "100/669 [===>..........................] - ETA: 49s - loss: 48.6131 - g_loss: 17.6850 - d_loss: 30.9281 WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Discriminator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 9/9\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000025260D339D8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000025260D339D8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      " 90/669 [===>..........................] - ETA: 45s - loss: 51.0538 - g_loss: 16.1813 - d_loss: 34.8725\n",
      "Epoch 00009: saving model to model_checkpoint/model_9_gloss_16.6126_dloss_34.3125.h5\n",
      "100/669 [===>..........................] - ETA: 41s - loss: 50.9251 - g_loss: 16.6126 - d_loss: 34.3125Epoch 11/15\n",
      "WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Generator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 10/10\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525DED6400> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525DED6400> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "100/669 [===>..........................] - ETA: 46s - loss: 40.5590 - g_loss: 13.9269 - d_loss: 26.6320WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Discriminator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 10/10\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525AAC4268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525AAC4268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      " 90/669 [===>..........................] - ETA: 45s - loss: 48.3853 - g_loss: 19.1028 - d_loss: 29.2824 \n",
      "Epoch 00010: saving model to model_checkpoint/model_10_gloss_18.5449_dloss_30.3829.h5\n",
      "100/669 [===>..........................] - ETA: 41s - loss: 48.9279 - g_loss: 18.5449 - d_loss: 30.3829Epoch 12/15\n",
      "WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Generator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 11/11\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000025261417AE8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000025261417AE8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "100/669 [===>..........................] - ETA: 47s - loss: 49.7556 - g_loss: 18.0057 - d_loss: 31.7499 WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Discriminator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 11/11\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525AA0B1E0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525AA0B1E0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      " 90/669 [===>..........................] - ETA: 43s - loss: 43.9894 - g_loss: 13.2838 - d_loss: 30.7056\n",
      "Epoch 00011: saving model to model_checkpoint/model_11_gloss_14.2588_dloss_29.9995.h5\n",
      "100/669 [===>..........................] - ETA: 39s - loss: 44.2583 - g_loss: 14.2588 - d_loss: 29.9995Epoch 13/15\n",
      "WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Generator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 12/12\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525DC02400> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525DC02400> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "100/669 [===>..........................] - ETA: 50s - loss: 43.8055 - g_loss: 16.1285 - d_loss: 27.6771WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Discriminator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 12/12\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525D545730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525D545730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      " 90/669 [===>..........................] - ETA: 43s - loss: 48.2328 - g_loss: 20.6840 - d_loss: 27.5488 \n",
      "Epoch 00012: saving model to model_checkpoint/model_12_gloss_19.9161_dloss_27.3126.h5\n",
      "100/669 [===>..........................] - ETA: 40s - loss: 47.2288 - g_loss: 19.9161 - d_loss: 27.3126Epoch 14/15\n",
      "WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Generator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 13/13\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525D58A2F0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525D58A2F0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "100/669 [===>..........................] - ETA: 50s - loss: 51.0878 - g_loss: 18.9695 - d_loss: 32.1183WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Discriminator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 13/13\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525B5EC620> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525B5EC620> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      " 90/669 [===>..........................] - ETA: 44s - loss: 44.5972 - g_loss: 15.3043 - d_loss: 29.2929\n",
      "Epoch 00013: saving model to model_checkpoint/model_13_gloss_15.9985_dloss_29.8195.h5\n",
      "100/669 [===>..........................] - ETA: 40s - loss: 45.8180 - g_loss: 15.9985 - d_loss: 29.8195Epoch 15/15\n",
      "WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Generator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 14/14\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000025261AFA620> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000025261AFA620> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      " 90/669 [===>..........................] - ETA: 54s - loss: 47.3715 - g_loss: 17.3467 - d_loss: 30.0247 WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Discriminator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 14/14\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525B043B70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002525B043B70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      " 90/669 [===>..........................] - ETA: 42s - loss: 49.7138 - g_loss: 22.4148 - d_loss: 27.2990 \n",
      "Epoch 00014: saving model to model_checkpoint/model_14_gloss_22.8490_dloss_27.7742.h5\n",
      "100/669 [===>..........................] - ETA: 39s - loss: 50.6232 - g_loss: 22.8490 - d_loss: 27.7742"
     ]
    }
   ],
   "source": [
    "adae = ADAE(data_size=(11,))\n",
    "adae.train(x1_train.values, x2_train.values, x1_test.values, x2_test.values, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(x_train.values))\n",
    "print(np.shape(y_train.values))\n",
    "print(np.shape(x_test.values))\n",
    "print(np.shape(y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312    experiment_101725_files\n",
       "606    experiment_101725_files\n",
       "440    experiment_101725_files\n",
       "1      experiment_101725_files\n",
       "317    experiment_101725_files\n",
       "                ...           \n",
       "107    experiment_101725_files\n",
       "271    experiment_101725_files\n",
       "861    experiment_101725_files\n",
       "436    experiment_101725_files\n",
       "103    experiment_101725_files\n",
       "Name: batch, Length: 669, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
