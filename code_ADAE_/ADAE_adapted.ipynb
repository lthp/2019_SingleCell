{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import pdb\n",
    "\n",
    "from tensorflow.python.keras.layers import Input, Dense, Lambda, Flatten, Reshape, Concatenate\n",
    "from tensorflow.python.keras.layers import Conv2D, Conv2DTranspose, BatchNormalization, Activation, LeakyReLU\n",
    "from tensorflow.python.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.python.keras import metrics\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.python.keras.datasets import mnist\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#tensorflow.version\n",
    "\n",
    "def lr_scheduler(initial_lr=1e-3, decay_factor=0.75, step_size=5, min_lr=1e-5):\n",
    "    '''\n",
    "    Wrapper function to create a LearningRateScheduler with step decay schedule.\n",
    "    '''\n",
    "    def schedule(epoch):\n",
    "        lr = initial_lr * (decay_factor ** np.floor(epoch / step_size))\n",
    "        if lr > min_lr:\n",
    "            return lr\n",
    "        return min_lr\n",
    "\n",
    "    return LearningRateScheduler(schedule, verbose=1)\n",
    "\n",
    "\n",
    "class Autoencoder():\n",
    "    def __init__(self, shape_data = (11,1), intermediate_dim=128, latent_dim=32):\n",
    "        self.data_size = shape_data\n",
    "        #self.intermediate_dim = intermediate_dim\n",
    "        #self.latent_dim = latent_dim\n",
    "        self.input = Input(shape=self.data_size)\n",
    "        #self.params = {\n",
    "#             'enc_filters': [16, 32, 48, 16, 32],\n",
    "#             'enc_kernels': [3, 3, 3, 4, 1],\n",
    "#             'enc_strides': [2, 2, 2, 1, 1],\n",
    "#             'dec_filters': [16, 16, 16, 32],\n",
    "#             'dec_kernels': [4, 3, 3, 3],\n",
    "#             'dec_strides': [1, 2, 2, 2],\n",
    "#         }\n",
    "        self.reconstruction_shape = []\n",
    "        self.params = {'enc_dim': [8, 4] , 'dec_dim': [4, 8, 11] }\n",
    "\n",
    "    def build_encoder(self, dims=[8, 4]): #, kernels=[3, 3, 3, 4, 1], strides=[2, 2, 2, 1, 1]):\n",
    "        def f(x):\n",
    "            for intermediate_dim in dims: #num_filter, kernel, stride in zip(filters, kernels, strides):\n",
    "                x = Dense( units=intermediate_dim,\n",
    "                           activation=tf.nn.sigmoid)(x)\n",
    "                x = BatchNormalization()(x)\n",
    "                self.reconstruction_shape += [x.get_shape().as_list()]\n",
    "            return x\n",
    "        return f\n",
    "\n",
    "    def build_decoder(self, dims=[4, 8, 11]): #filters=[64, 64, 64, 32], kernels=[4, 3, 3, 3], strides=[1, 2, 2, 2]):\n",
    "        def f(x):\n",
    "            for intermediate_dim in dims: #for i, (num_filter, kernel, stride) in enumerate(zip(filters, kernels, strides)):\n",
    "                x = Dense(units=intermediate_dim,\n",
    "                           activation=tf.nn.sigmoid)(x)\n",
    "                x = BatchNormalization()(x)\n",
    "                decoder = x\n",
    "            return decoder\n",
    "        return f\n",
    "\n",
    "    def build_model(self):\n",
    "        hidden = self.build_encoder(\n",
    "            dims=self.params['enc_dim'])(self.input)\n",
    "        dec = self.build_decoder(\n",
    "            dims=self.params['dec_dim'])(hidden)\n",
    "        # instantiate VAE model\n",
    "        vae = Model(self.input, dec)\n",
    "        return vae\n",
    "\n",
    "\n",
    "class ADAE(object):\n",
    "    def __init__(self, data_size = (11, 1)):#image_size=(28, 28, 1), latent_dim=100):\n",
    "        self.data_size = data_size\n",
    "        #self.latent_dim = latent_dim\n",
    "\n",
    "        self.input = Input(shape=data_size)\n",
    "        # Build the generator\n",
    "        self.generator = Autoencoder(shape_data = data_size).build_model()\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = Autoencoder(shape_data = data_size).build_model()\n",
    "\n",
    "        self.gx = self.generator(self.input)\n",
    "        self.dx = self.discriminator(self.input)\n",
    "        self.dgx = self.discriminator(self.gx)\n",
    "        \n",
    "        \n",
    "\n",
    "        print(self.gx.shape, self.dx.shape, self.dgx.shape, self.input.shape)\n",
    "        self.d_loss = Lambda(lambda x: K.mean(K.abs(x[0] - x[1]), axis=1) -\n",
    "                             K.mean(K.abs(x[2] - x[3]), axis=1), name='d_loss')([self.input, self.dx, self.gx, self.dgx])\n",
    "        \n",
    "        \n",
    "        self.g_loss = Lambda(lambda x: K.mean(K.abs(x[0] - x[1]), axis=1) +\n",
    "                            K.mean(K.abs(x[1] - x[2]), axis=1), name='g_loss')([self.input, self.gx, self.dgx])\n",
    "        self.model = Model(inputs=[self.input], outputs=[self.g_loss, self.d_loss])\n",
    "\n",
    "        self.model.summary()\n",
    "        # self.generator.summary()\n",
    "        # self.discriminator.summary()\n",
    "\n",
    "#     def get_anomaly_score(self):\n",
    "#         \"\"\" Compute the anomaly score. Call it after training. \"\"\"\n",
    "#         score_out = Lambda(lambda x:\n",
    "#                            K.mean(K.mean(K.mean((x[0] - x[1]) ** 2, axis=1), axis=1), axis=1)\n",
    "#                            )([self.model.inputs[0], self.model.layers[2](self.model.layers[1](self.model.inputs[0]))])\n",
    "#         return Model(self.model.inputs[0], score_out)\n",
    "\n",
    "    def get_generator_trained_model(self):\n",
    "        \"\"\" Get the generator to reconstruct the input. Call it after training. \"\"\"\n",
    "        return Model(self.model.inputs[0], self.model.layers[1](self.model.inputs[0]))\n",
    "\n",
    "    def get_discrinminator_trained_model(self):\n",
    "        \"\"\" Get the discrinminator to reconstruct the input. Call it after training. \"\"\"\n",
    "        return Model(self.model.inputs[0], self.model.layers[2](self.model.layers[1](self.model.inputs[0])))\n",
    "\n",
    "    def train(self, x_train, x_test, y_train, y_test, epochs=1):\n",
    "        self.model.add_loss(K.mean(self.g_loss))\n",
    "        self.model.add_metric(self.g_loss, aggregation='mean', name=\"g_loss\")\n",
    "        self.model.add_loss(K.mean(self.d_loss))\n",
    "        self.model.add_metric(self.d_loss, aggregation='mean', name=\"d_loss\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print('Epoch %d/%d' % (epoch + 1, epochs))\n",
    "            # Train generator only\n",
    "            self.model.layers[1].trainable = True\n",
    "            self.model.layers[2].trainable = False\n",
    "            self.model.compile('adam', loss_weights={'g_loss': 1, 'd_loss': 0})\n",
    "            print('Training on Generator')\n",
    "            self.model.fit(\n",
    "                x_train,\n",
    "                batch_size=10, #64,\n",
    "                steps_per_epoch=10, #200,\n",
    "                epochs=epoch,\n",
    "                callbacks=[\n",
    "\n",
    "                        lr_scheduler(initial_lr=1e-3, decay_factor=0.75, step_size=10, min_lr=1e-5)\n",
    "                    \n",
    "                ],\n",
    "                initial_epoch=epoch - 1\n",
    "            )\n",
    "\n",
    "            # Train discriminator only\n",
    "            self.model.layers[1].trainable = False\n",
    "            self.model.layers[2].trainable = True\n",
    "            self.model.compile('adam', loss_weights={'g_loss': 0, 'd_loss': 1})\n",
    "            print('Training on Discriminator')\n",
    "            self.model.fit(\n",
    "                x_train,\n",
    "                batch_size=10, #64,\n",
    "                steps_per_epoch=10, #200,\n",
    "                epochs=epoch,\n",
    "                callbacks=[\n",
    "                    ModelCheckpoint(\n",
    "                        './model_checkpoint/model_%d_gloss_{g_loss:.4f}_dloss_{d_loss:.4f}.h5' % epoch,\n",
    "                        verbose=1\n",
    "                    ),\n",
    "\n",
    "                        lr_scheduler(initial_lr=1e-3, decay_factor=0.75, step_size=10, min_lr=1e-5)\n",
    "                    \n",
    "                ],\n",
    "                initial_epoch=epoch - 1\n",
    "            )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \n",
    "#path = '/Users/laurieprelot/Documents/Projects/2019_Deep_learning/data/Chevrier-et-al/chevrier_data_pooled_nona.parquet'\n",
    "path = '/Users/joannaf/Desktop/courses/DeepLearning/DL2019/project/data/Dataset5/chevrier_data_pooled_nona.parquet'\n",
    "chve = pd.read_parquet(path, engine='pyarrow')\n",
    "chve.shape\n",
    "#np.random.seed(123456)\n",
    "#idx = np.random.choice(a = np.arange(chve.shape[0]), size = 2000, replace = False)\n",
    "#chve_s = #chve.iloc[idx, ]\n",
    "ID = 'rcc7'\n",
    "select_cols = [col for col in chve.columns if not \"metadata\" in col]\n",
    "chve = chve.loc[:,select_cols]\n",
    "chve_s_patient = chve.reset_index()\n",
    "chve_s_patient = chve_s_patient.rename({'level_0':'batch', 'level_1':'patient', 'level_2':'cell'} , axis = 1)\n",
    "chve_s_patient = chve_s_patient.loc[chve_s_patient['patient'] == ID, :]\n",
    "\n",
    "chve_s_patient_batch1 = chve_s_patient.loc[ chve_s_patient['batch'] == \"experiment_101725_files\"]\n",
    "\n",
    "# target, split\n",
    "chve_s_patient_batch1 = chve_s_patient_batch1.iloc[1:1000,:]\n",
    "y = chve_s_patient_batch1[\"batch\"]\n",
    "x = chve_s_patient_batch1.drop([\"batch\", 'cell', 'patient'], axis = 1) \n",
    "x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.33, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 14)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chve_s_patient_batch1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 11) (None, 11) (None, 11) (None, 11)\n",
      "Model: \"model_41\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_40 (InputLayer)           [(None, 11)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_39 (Model)                (None, 11)           431         input_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_40 (Model)                (None, 11)           431         input_40[0][0]                   \n",
      "                                                                 model_39[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "g_loss (Lambda)                 (None,)              0           input_40[0][0]                   \n",
      "                                                                 model_39[1][0]                   \n",
      "                                                                 model_40[2][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "d_loss (Lambda)                 (None,)              0           input_40[0][0]                   \n",
      "                                                                 model_40[1][0]                   \n",
      "                                                                 model_39[1][0]                   \n",
      "                                                                 model_40[2][0]                   \n",
      "==================================================================================================\n",
      "Total params: 862\n",
      "Trainable params: 722\n",
      "Non-trainable params: 140\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Generator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00000: LearningRateScheduler reducing learning rate to 0.0013333333333333333.\n",
      " 10/669 [..............................] - ETA: 3:17 - loss: 21.7264 - g_loss: 11.9368 - d_loss: 9.7897WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Discriminator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00000: LearningRateScheduler reducing learning rate to 0.0013333333333333333.\n",
      " 10/669 [..............................] - ETA: 3:26 - loss: 38.2441 - g_loss: 20.1191 - d_loss: 18.1250\n",
      "Epoch 00000: saving model to ./model_checkpoint/model_0_gloss_17.7730_dloss_15.7060.h5\n",
      "100/669 [===>..........................] - ETA: 18s - loss: 33.4789 - g_loss: 17.7730 - d_loss: 15.7060 Epoch 2/5\n",
      "WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Generator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
      " 10/669 [..............................] - ETA: 3:12 - loss: 36.8773 - g_loss: 19.4578 - d_loss: 17.4195WARNING:tensorflow:Output g_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to g_loss.\n",
      "WARNING:tensorflow:Output d_loss missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to d_loss.\n",
      "Training on Discriminator\n",
      "Train on 669 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
      " 10/669 [..............................] - ETA: 3:22 - loss: 24.5175 - g_loss: 13.2147 - d_loss: 11.3028"
     ]
    }
   ],
   "source": [
    "adae = ADAE(data_size=(11,))\n",
    "adae.train(x_train.values , x_test.values, y_train.values, y_test.values, epochs=5)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
