{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import glob\n",
    "import pdb\n",
    "import scipy as sp\n",
    "from universal_divergence import estimate\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from math import log, e\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### copied from BERMUDA and ensured the random seeds are set where appropriate (= slightly modified)\n",
    "def cal_UMAP(code, pca_dim = 50, n_neighbors = 30, min_dist=0.1, n_components=2, metric='cosine', random_state=0):\n",
    "    \"\"\" Calculate UMAP dimensionality reduction\n",
    "    Args:\n",
    "        code: num_cells * num_features\n",
    "        pca_dim: if dimensionality of code > pca_dim, apply PCA first\n",
    "        n_neighbors: UMAP parameter\n",
    "        min_dist: UMAP parameter\n",
    "        n_components: UMAP parameter\n",
    "        metric: UMAP parameter\n",
    "        random_state: random seed\n",
    "    Returns:\n",
    "        umap_code: num_cells * n_components\n",
    "    \"\"\"\n",
    "    if code.shape[1] > pca_dim:\n",
    "        pca = PCA(n_components=pca_dim)\n",
    "        code = pca.fit_transform(code)\n",
    "    fit = umap.UMAP(n_neighbors=n_neighbors,\n",
    "                    min_dist=min_dist,\n",
    "                    n_components=n_components,\n",
    "                    metric=metric,\n",
    "                    random_state=random_state)\n",
    "    umap_code = fit.fit_transform(code)\n",
    "\n",
    "    return umap_code\n",
    "\n",
    "def entropy(labels, base=None):\n",
    "    \"\"\" Computes entropy of label distribution.\n",
    "    Args:\n",
    "        labels: list of integers\n",
    "    Returns:\n",
    "        ent: entropy\n",
    "    \"\"\"\n",
    "    n_labels = len(labels)\n",
    "    if n_labels <= 1:\n",
    "        return 0\n",
    "    value, counts = np.unique(labels, return_counts=True)\n",
    "    probs = counts / n_labels\n",
    "    n_classes = np.count_nonzero(probs)\n",
    "    if n_classes <= 1:\n",
    "        return 0\n",
    "    ent = 0\n",
    "    # Compute entropy\n",
    "    base = e if base is None else base\n",
    "    for i in probs:\n",
    "        ent -= i * log(i, base)\n",
    "    return ent\n",
    "\n",
    "\n",
    "def cal_entropy(code, idx, dataset_labels, k=100):\n",
    "    \"\"\" Calculate entropy of cell types of nearest neighbors\n",
    "    Args:\n",
    "        code: num_cells * num_features, embedding for calculating entropy\n",
    "        idx: binary, index of observations to calculate entropy\n",
    "        dataset_labels:\n",
    "        k: number of nearest neighbors\n",
    "    Returns:\n",
    "        entropy_list: list of entropy of each cell\n",
    "    \"\"\"\n",
    "    cell_sample = np.where(idx == True)[0]\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='kd_tree').fit(code)\n",
    "    entropy_list = []\n",
    "    _, indices = nbrs.kneighbors(code[cell_sample, :])\n",
    "    for i in range(len(cell_sample)):\n",
    "        entropy_list.append(entropy(dataset_labels[indices[i, :]]))\n",
    "\n",
    "    return entropy_list\n",
    "\n",
    "\n",
    "def evaluate_scores(div_ent_code, sil_code, cell_labels, dataset_labels, num_datasets,\n",
    "                    div_ent_dim, sil_dim, sil_dist, random_state, cal_min=30):\n",
    "    \"\"\" Calculate three proposed evaluation metrics\n",
    "    Args:\n",
    "        div_ent_code: num_cells * num_features, embedding for divergence and entropy calculation, usually with dim of 2\n",
    "        sil_code: num_cells * num_features, embedding for silhouette score calculation\n",
    "        cell_labels:\n",
    "        dataset_labels:\n",
    "        num_datasets:\n",
    "        div_ent_dim: if dimension of div_ent_code > div_ent_dim, apply PCA first\n",
    "        sil_dim: if dimension of sil_code > sil_dim, apply PCA first\n",
    "        sil_dist: distance metric for silhouette score calculation\n",
    "        random_state: random seed\n",
    "        cal_min: minimum number of cells for estimation\n",
    "    Returns:\n",
    "        div_score: divergence score\n",
    "        ent_score: entropy score\n",
    "        sil_score: silhouette score\n",
    "    \"\"\"\n",
    "    # calculate divergence and entropy\n",
    "    if div_ent_code.shape[1] > div_ent_dim:\n",
    "        div_ent_code = PCA(n_components=div_ent_dim, random_state=random_state).fit_transform(div_ent_code)\n",
    "    div_pq = []  # divergence dataset p, q\n",
    "    div_qp = []  # divergence dataset q, p\n",
    "    ent = []  # entropy\n",
    "    # pairs of datasets\n",
    "    for d1 in range(1, num_datasets+1):\n",
    "        for d2 in range(d1+1, num_datasets+1):\n",
    "            idx1 = dataset_labels == d1\n",
    "            idx2 = dataset_labels == d2\n",
    "            labels = np.intersect1d(np.unique(cell_labels[idx1]), np.unique(cell_labels[idx2]))\n",
    "            idx1_mutual = np.logical_and(idx1, np.isin(cell_labels, labels))\n",
    "            idx2_mutual = np.logical_and(idx2, np.isin(cell_labels, labels))\n",
    "            idx_specific = np.logical_and(np.logical_or(idx1, idx2), np.logical_not(np.isin(cell_labels, labels)))\n",
    "            # divergence\n",
    "            if np.sum(idx1_mutual) >= cal_min and np.sum(idx2_mutual) >= cal_min:\n",
    "                div_pq.append(max(estimate(div_ent_code[idx1_mutual, :], div_ent_code[idx2_mutual, :], cal_min), 0))\n",
    "                div_qp.append(max(estimate(div_ent_code[idx2_mutual, :], div_ent_code[idx1_mutual, :], cal_min), 0))\n",
    "            # entropy\n",
    "            if (sum(idx_specific) > 0):\n",
    "                ent_tmp = cal_entropy(div_ent_code, idx_specific, dataset_labels)\n",
    "                ent.append(sum(ent_tmp) / len(ent_tmp))\n",
    "    if len(ent) == 0:  # if no dataset specific cell types, store entropy as -1\n",
    "        ent.append(-1)\n",
    "\n",
    "    # calculate silhouette_score (only if more than one cell-type provided)\n",
    "    if(len(set(cell_labels))==1):\n",
    "        sil_scores = [np.nan]\n",
    "    else:  \n",
    "        if sil_code.shape[1] > sil_dim:\n",
    "            sil_code = PCA(n_components=sil_dim,random_state=random_state).fit_transform(sil_code)\n",
    "        sil_scores = silhouette_samples(sil_code, cell_labels, metric=sil_dist)\n",
    "\n",
    "    # average for scores\n",
    "    div_score = (sum(div_pq) / len(div_pq) + sum(div_qp) / len(div_qp)) / 2\n",
    "    ent_score = sum(ent) / len(ent)\n",
    "    sil_score = sum(sil_scores) / len(sil_scores)\n",
    "\n",
    "    return div_score, ent_score, sil_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to convert the model output into BERMUDA-required format for evaluation\n",
    "def separate_metadata(data):\n",
    "    \"\"\"\n",
    "    Function to create metadata from data index\n",
    "    inputs:\n",
    "        pandas dataframe with all metainfo in the index, separated by '_'\n",
    "    outputs:\n",
    "        data, metadata\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    assert(df.index.nlevels==1)\n",
    "    metainfo = df.index.to_frame(name='metainfo').metainfo.str.split(\"_\", n = -1, expand = True) \n",
    "    metainfo.columns = [\"\".join(e for e in x if e.isalpha()) for x in metainfo.iloc[0,:]]\n",
    "    if(len([x for x in metainfo.columns if 'celltype' in x])):\n",
    "        metainfo.columns = ['cell_type' if 'celltype' in x else x for x in metainfo.columns]\n",
    "        metainfo['cell_type'] = [x.split('celltype')[-1] for x in metainfo['cell_type']]\n",
    "    else:\n",
    "        metainfo['cell_type'] = 'all'\n",
    "    df.index = [t+'_cell'+str(i) for (i,t) in enumerate(df.index)]\n",
    "    metainfo.index = df.index\n",
    "    return(df, metainfo)\n",
    "\n",
    "def prep_data_for_eval(data, metadata, umap_dim=20, random_state=0):\n",
    "    \"\"\"\n",
    "    Function to convert the data and metadata into a format required by the vealuation functions\n",
    "    inputs:\n",
    "        data: (batch-corrected) data containing all cells\n",
    "        metadata: corresopnding metadata with index shared with data, column 'batch' and column 'cell_type'\n",
    "        random_state: random seed\n",
    "    outputs:\n",
    "        umap_codes, data, cell_type_labels, batch_labels, number_of_datasets\n",
    "    \"\"\"\n",
    "    assert(data.index.nlevels==1)\n",
    "    idx = list(data.index)\n",
    "    # get batch labels\n",
    "    batch_labels = metadata.loc[idx,'batch']\n",
    "    ct_labels = metadata.loc[idx,'cell_type']\n",
    "    \n",
    "    num_datasets = len(set(batch_labels))\n",
    "    batch_dict = dict(zip(set(batch_labels), range(len(set(batch_labels)))))\n",
    "    batch_labels_num = np.array([batch_dict[x]+1 for x in batch_labels])\n",
    "    ct_dict = dict(zip(set(ct_labels), range(len(set(ct_labels)))))\n",
    "    ct_labels_num = np.array([ct_dict[x]+1 for x in ct_labels])\n",
    "    \n",
    "    data = np.array(data)\n",
    "    umap_codes = cal_UMAP(data, umap_dim, random_state=random_state)\n",
    "\n",
    "    return(umap_codes, data, ct_labels_num, batch_labels_num, num_datasets)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### exemplary workflow (in reality df is the model output)\n",
    "\n",
    "#os.chdir('/Users/joannaf/Desktop/courses/DeepLearning/DL2019/project/data/simulated/')\n",
    "random_state = 345\n",
    "np.random.seed(random_state)\n",
    "df = pd.read_parquet('toy_data_gamma_small.parquet')\n",
    "selected_cells = np.random.choice(range(df.shape[0]), 1000)\n",
    "df = df.iloc[selected_cells,:]\n",
    "df = df.loc[:,~df.columns.str.startswith('meta')]\n",
    "# if want to have cell_types included, run:\n",
    "cts = ['Tcell','Bcell','Tumorcell','Fibroblast','Endothelial']*int(df.shape[0]/5)\n",
    "df.index = [x+'_celltype'+cts[i] for (i,x) in enumerate(df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### exemplary workflow\n",
    "umap_pca_dim = 50\n",
    "div_ent_dim = 50\n",
    "sil_dim = 50 #df.shape[1]\n",
    "\n",
    "### df is the model output\n",
    "df, metadf = separate_metadata(df)\n",
    "### prepare data for evaluation\n",
    "umap_codes, data, cell_type_labels, batch_labels, num_datasets = prep_data_for_eval(df, metadf, umap_pca_dim,random_state=random_state)\n",
    "### evaluate performance\n",
    "divergence_score, entropy_score, silhouette_score = evaluate_scores(umap_codes, data, cell_type_labels,\n",
    "                                                                    batch_labels, num_datasets,\n",
    "                                                                    div_ent_dim, sil_dim, 'cosine', \n",
    "                                                                    random_state = random_state)  \n",
    "divergence_score, entropy_score, silhouette_score\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
